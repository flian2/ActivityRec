{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from clean_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for person 4. Use sequence 1~3 for training, 4~5 for testing.\n",
    "person = 4\n",
    "sadl_n = []\n",
    "for n in range(1, 6):\n",
    "    sadl_n.append(pd.read_table('data/S%d-ADL%d.dat' % (person, n), sep='\\s+', header=None, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Smooth data, time: col 0, features: col 1~36, labels: col 244 \n",
    "winsize = 15\n",
    "stepsize = 8\n",
    "\n",
    "# train data\n",
    "train_sample = np.empty((0, 36))\n",
    "train_labels = np.empty((0))\n",
    "train_len = []\n",
    "for i in range(0, 3):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    train_sample = np.concatenate((train_sample, features), axis=0)\n",
    "    train_len.append(features.shape[0])\n",
    "    train_labels = np.concatenate( (train_labels, labels) )\n",
    "train_len = np.array(train_len)\n",
    "\n",
    "print \"train_sample size: \", train_sample.shape\n",
    "print \"train_labels size: \", train_labels.shape\n",
    "print \"subsequence length: \", train_len, \". Sum of length: \", np.sum(train_len)\n",
    "\n",
    "# test data\n",
    "test_sample = np.empty((0, 36))\n",
    "test_labels = np.empty((0))\n",
    "test_len = []\n",
    "for i in range(3, 5):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    test_sample = np.concatenate((test_sample, features), axis=0)\n",
    "    test_len.append(features.shape[0])\n",
    "    test_labels = np.concatenate( (test_labels, labels) )\n",
    "test_len = np.array(test_len)  \n",
    "\n",
    "print \"test_sample size: \", test_sample.shape\n",
    "print \"test_labels size: \", test_labels.shape\n",
    "print \"subsequence length: \", test_len, \". Sum of length: \", np.sum(test_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "col_threshold = 0.5\n",
    "train, test = fill_missing(train_sample, test_sample, col_threshold, True)\n",
    "np.any(np.isnan(train)), np.any(np.isnan(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler() # center to mean and normalize to unit variance\n",
    "train_normalized = scalar.fit_transform(train)\n",
    "test_normalized = scalar.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_normalized)\n",
    "var_thres = 0.95 # keep components to up to 95% total variance\n",
    "n_comp = (pca.explained_variance_ratio_.cumsum() < var_thres).sum() + 1\n",
    "print \"Keep %d compoments to retrain %f variance\" % (n_comp, var_thres)\n",
    "\n",
    "pca_train = PCA(n_components=n_comp)\n",
    "train_reduced = pca_train.fit_transform(train_normalized)\n",
    "test_reduced = pca_train.transform(test_normalized)\n",
    "print \"Size of reduced dimension training data: \", train_reduced.shape\n",
    "print \"Size of reduced dimension testing data: \", test_reduced.shape\n",
    "# Cleaned data: train_reduced, test_reduced, train_labels, test_labels, train_len, test_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize upper level left-right hmm model with 6 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hmm import DiscreteDistr, GaussDistr, GaussMixDistr\n",
    "from hmm import MarkovChain\n",
    "from hmm import HMM, make_leftright_hmm\n",
    "\n",
    "# Build a left-right hmm with 6 states, discrete output distribution, the output is the discrete label of activity\n",
    "label_transfer = (np.maximum(train_labels - 100, 0) + 1)[:, np.newaxis]\n",
    "# transform the labels into range 1~6. {0: 1, 101: 2, 102: 3, 103: 4, 104: 5, 105: 6}\n",
    "discreteD = DiscreteDistr(np.ones((6))) # a discrete distribution with 6 possible output\n",
    "n_states = 6\n",
    "\n",
    "hmm_state = make_leftright_hmm(n_states, discreteD, obs_data=label_transfer, l_data=train_len)\n",
    "print \"Probability mass P(label | state): \"\n",
    "prob_mass = np.zeros((n_states, n_states))\n",
    "for i in range(0, n_states):\n",
    "    prob_mass[i, :] = hmm_state.output_distr[i].prob_mass\n",
    "prob_mass[prob_mass < 1e-2] = 0\n",
    "for i in range(0, n_states):\n",
    "    hmm_state.output_distr[i].prob_mass = prob_mass[i, :]\n",
    "print prob_mass\n",
    "\n",
    "# Assign max probability activity label to each state\n",
    "# If multiple states are assigned with one same label, then inside each of these states use a small hmm \n",
    "state_act_label = np.array([np.argmax(prob_mass[i, :]) for i in range(0, n_states)])\n",
    "act_label_count = np.zeros((n_states)) # how many states correspond to label i\n",
    "for i in range(0, n_states):\n",
    "    act_label_count[i] = np.sum(state_act_label == i)\n",
    "print \"The maximum likelihood label for each state: \"\n",
    "print state_act_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train output distribution\n",
    "def search_num_mixtures_likelihood(features, train_len, n_mix):\n",
    "    # Use cross validation on loglikehood to find the optimal number of mixtures for each class label\n",
    "    # return: \n",
    "    # gmm: the gmm model that gives the highest mean logprob\n",
    "    # N: number of gaussian mixtures in the gmm\n",
    "    # logp_x: mean of logP(Xt | gmm) for the chosen gmm\n",
    "    mean_logprob = [0 for i in range(0, len(n_mix))] # mean of logP(Xt | gmm)\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    for i, N in enumerate(n_mix):\n",
    "        # K-fold cross validataion, each fold is one subsequence\n",
    "        n_folds = len(train_len)\n",
    "        likelihood_scores = []\n",
    "        for k in range(0, n_folds):\n",
    "            val_mask = np.zeros((features.shape[0])).astype(bool)\n",
    "            val_mask[start_ind[k]: start_ind[k] + train_len[k]] = True\n",
    "            train_mask = np.logical_not(val_mask)\n",
    "            gmm = GaussMixDistr(gauss=N)\n",
    "            gmm.init_by_data(features[train_mask, :])\n",
    "            gmm.train(features[train_mask, :])\n",
    "            logprob_x = gmm.logprob(features[val_mask, :])\n",
    "            likelihood_scores.append(np.mean(logprob_x))    \n",
    "        mean_logprob[i] = np.mean(likelihood_scores)\n",
    "    # Refit gmm using all data with selected number of mixtures\n",
    "    gmm_list = [] # one gmm model for each \"number of components\"\n",
    "    i_gmm = np.argmax(mean_logprob)\n",
    "    n_components = n_mix[i_gmm]\n",
    "    gmm_opt = GaussMixDistr(gauss=n_components)\n",
    "    gmm_opt.init_by_data(features)\n",
    "    gmm_opt.train(features)\n",
    "    likelihood_score = np.mean(gmm_opt.logprob(features))\n",
    "    return gmm_opt, n_components, likelihood_score\n",
    "\n",
    "def make_outputdistr(train, train_len, train_labels, class2label):\n",
    "    outputdistr_stats = pd.DataFrame(index=class2label, columns=['n_mixture', 'mean_loglikelihood'])\n",
    "    outputdistr_gmm = []\n",
    "    n_mix = range(1, 11)\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    for i, label in enumerate(class2label):\n",
    "        per_label_trainlen = [sum(train_labels[start_ind[k]: start_ind[k] + length] == label) for k, length in enumerate(train_len)]\n",
    "        gmm, N, logprob = search_num_mixtures_likelihood(train[train_labels == label], per_label_trainlen, n_mix)\n",
    "        outputdistr_gmm.append(gmm)\n",
    "        outputdistr_stats.iloc[i, :] = [N, logprob]\n",
    "    return outputdistr_gmm, outputdistr_stats\n",
    "\n",
    "class2label = [0, 101, 102, 103, 104, 105]\n",
    "outputdistr_gmm, outputdistr_stats = make_outputdistr(train_reduced, train_len, train_labels, class2label)\n",
    "outputdistr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglikelihood(mc, x, prob_mass, gmms):\n",
    "    \"\"\"\n",
    "    Compute log likelihood for each observation sample given the MarkovChain of top-layer hmm, \n",
    "    and gmm distribution P(X(t) | activity label = i).\n",
    "    Input:\n",
    "    mc: MarkovChain for top-layer hmm. \n",
    "    prob_mass: probability mass for output distribution of top-layer hmm. \n",
    "               prob_mass[i, j] = P[activity_label = j | S_{hmm_top} = i]\n",
    "    gmms: list of gmm object. gmms[i] has P(X(t) | activity_label = i)\n",
    "    Return:\n",
    "    logp_x: [n_states, n_samples]. logp_x[i, t] = log P[X(t) | S_{hmm_top} = i]\n",
    "    Method:\n",
    "    Compute: P(X(t) | state = i ) for t = 0...T, i = 0...5,\n",
    "    P(X(t) | state = i ) = \\sum_{label i} P(X(t) | label = j, state = i) * P(label = j | state = i)\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    n_states = mc.n_states\n",
    "    logp_x = np.zeros((n_states, T))\n",
    "    for state in range(0, mc.n_states):\n",
    "        label_ind = np.argwhere(prob_mass[state, :] > 0)\n",
    "        p0 = prob_mass[state, prob_mass[state, :] > 0]\n",
    "        logprob_per_label = gmms[0].logprob(x, [gmms[i] for i in label_ind])\n",
    "        logp_x[state, :] = np.log( p0[np.newaxis, :].dot(np.exp(logprob_per_label)) )[0, :]\n",
    "\n",
    "    return logp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi_state_sequence(mc, x, x_len, prob_mass, gmms):\n",
    "    \"\"\"\n",
    "    Predict top-layer hmm state sequence using viterbi algorithm.\n",
    "    Input\n",
    "    x: [T, data_size]. vector sequence stacked together\n",
    "    x_len: length of subsequences\n",
    "    mc:        MarkovChain for top-layer hmm.\n",
    "    prob_mass: probability mass for output distribution of top-layer hmm. \n",
    "               prob_mass[i, j] = P[activity_label = j | S_{hmm_top} = i]\n",
    "    gmms:      list of gmm object. gmms[i] has P(X(t) | activity_label = i)\n",
    "    Return:\n",
    "    s_opt: [T, ] predicted top-layer hmm state sequence\n",
    "    logP: [n_seq, ] logP of each subsequence\n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    s_opt = np.zeros((x.shape[0]))\n",
    "    logP = np.zeros((len(x_len)))\n",
    "    for i in range(0, len(x_len)):\n",
    "        logp_x = loglikelihood(mc, x[start_ind:start_ind + x_len[i], :], prob_mass, gmms)\n",
    "        s_opt[start_ind: start_ind + x_len[i]], logP[i] = mc.viterbi(logp_x)\n",
    "        start_ind += x_len[i]\n",
    "    return s_opt - 1, logP # the state sequence index from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict label within each state\n",
    "# Make an ergordic hmm with output distribution 1.0, (1-to-1 mapping between label and substate)\n",
    "\n",
    "train_states, _ = hmm_state.viterbi(label_transfer)\n",
    "train_states -= 1\n",
    "\n",
    "# train_states, _ = viterbi_state_sequence(hmm_state.state_gen, train_reduced, train_len, prob_mass, outputdistr_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sub_hmm(train_states, train_len, train_labels, n_states):\n",
    "    \"\"\"\n",
    "    Initialize and train the sub-layer hmm for each top-layer hmm state.\n",
    "    For the sub-layer hmm, each state corresponds to one activity label. \n",
    "    We train the transition prob by running Baum-Weltch EM training, by initializing the output \n",
    "    probability mass as the diagonal matrix, to force one to one mapping between sub-layer state and label.\n",
    "    Input:\n",
    "    train_states: [n_samples, ]. Sequence of top-layer hmm states.\n",
    "    train_len: list. Length of subsequences.\n",
    "    train_labels: [n_samples, ]. Activity labels of training sequence. The range of each label must be in [0, n_states)\n",
    "    Return:\n",
    "    mc_per_state: list of MarkovChain objects. mc_per_state[i] is the MarkovChain for top-layer state i. \n",
    "    \"\"\"\n",
    "    train_labels = train_labels[:, np.newaxis]\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    mc_per_state = []\n",
    "    for n in range(0, n_states):\n",
    "        x_labels = train_labels[train_states == n, :]\n",
    "        x_len = np.array([np.sum(train_states[s:s + train_len[i]] == n) for i, s in enumerate(start_ind)])\n",
    "        x_len = x_len[x_len > 0]\n",
    "        # Aii = 1 - 1/state_duration, Aij = 1/state_duration / (n_states_actual - 1)\n",
    "        A0 = np.eye((n_states))\n",
    "        D = np.array([np.sum(x_labels == i) / float(len(x_len)) for i in range(0, n_states)])\n",
    "        n_states_actual = np.sum(D > 0)\n",
    "        for i in range(0, n_states):\n",
    "            if D[i] == 0:\n",
    "                continue\n",
    "            A0[i, D > 0] = 1.0 / D[i] / n_states_actual\n",
    "            A0[i, i] = 1.0 - 1.0 / D[i]\n",
    "        p0 = np.ones((n_states)) / float(n_states_actual)\n",
    "        p0[D == 0] = 0.0\n",
    "        prob_mass = np.eye((n_states))\n",
    "        pD = [DiscreteDistr(prob_mass[i, :]) for i in range(0, n_states)]\n",
    "        mc = MarkovChain(p0, A0)\n",
    "        hmm_mc = HMM(mc, pD)\n",
    "        hmm_mc.train(obs_data=x_labels + 1, l_data=x_len) # discrete distribution index from 1 \n",
    "        mc_per_state.append(hmm_mc.state_gen)\n",
    "        \n",
    "    return mc_per_state\n",
    "    \n",
    "sub_mcs = make_sub_hmm(train_states, train_len, np.maximum(0, train_labels - 100), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict activity labels inside each high-level hmm state\n",
    "def predict_subhmm_labels(level_one_states, obs_data, l_data, sub_mcs, outputdistr):\n",
    "    # Input:\n",
    "    # level_one_states: [n_samples, ]. Sequences of states of high level hmm.\n",
    "    # obs_data: [n_samples, n_features]\n",
    "    # l_data: length of sequences. sum(l_data) = len(level_one_states)\n",
    "    # sub_mcs: MarkovChain objects for each high-level state. The order of states in the mc corresponds to the low-level label.\n",
    "    # outputdistr: the output distribution for the states in sub_mcs.\n",
    "    # Return:\n",
    "    # labels_opt: [n_samples, ]. predicted labels\n",
    "    start_ind = [0] + list(np.cumsum(l_data)[:-1].astype(int))\n",
    "    labels_opt = np.zeros((level_one_states.shape[0]))\n",
    "    cur_pos = 0\n",
    "    for t in range(0, len(l_data)):\n",
    "        state_subseq = level_one_states[start_ind[t]:start_ind[t] + l_data[t]]\n",
    "        obs_subseq = obs_data[start_ind[t]:start_ind[t] + l_data[t], :]\n",
    "        diff = np.append(np.array([1]), state_subseq[1:] - state_subseq[:-1])\n",
    "        i_newstate = np.append( np.argwhere(diff != 0), np.array([len(diff)]) )\n",
    "        for m in range(0, len(i_newstate) - 1):\n",
    "            state_tt = int(state_subseq[i_newstate[m]])\n",
    "            obs_tt = obs_subseq[i_newstate[m]:i_newstate[m + 1], :]\n",
    "            # run viterbi on markov chain sub_mcs[state_tt]\n",
    "            logp_x = outputdistr[0].logprob(obs_tt, outputdistr)\n",
    "            labels_tt, logP = sub_mcs[state_tt].viterbi(logp_x)\n",
    "            labels_opt[cur_pos:cur_pos + len(labels_tt)] = labels_tt - 1 # make the label index starting from 0\n",
    "            cur_pos += len(labels_tt)\n",
    "    return labels_opt   \n",
    "\n",
    "# Predict test labels\n",
    "test_states, _ = viterbi_state_sequence(hmm_state.state_gen, test_reduced, test_len, prob_mass, outputdistr_gmm)\n",
    "predicted_labels = predict_subhmm_labels(test_states, test_reduced, test_len, sub_mcs, outputdistr_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test with ADL4, ADL5\n",
    "true_labels = np.maximum(0, test_labels - 100)\n",
    "\n",
    "plt.plot(predicted_labels, '.', label=\"estimated label\")\n",
    "plt.plot(true_labels, '-', label=\"true label\")\n",
    "plt.plot(test_states, '--', label=\"high-level states\")\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('Activity label')\n",
    "plt.title(\"ADL4, ADL5 -- person %d\" % person)\n",
    "plt.legend(loc=9, bbox_to_anchor=(1.2, 1))\n",
    "plt.show()\n",
    "\n",
    "print \"accuracy: \", sum(predicted_labels == true_labels) / float(len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print \"Weighted F1 score: \", f1_score(true_labels, predicted_labels, average='weighted') \n",
    "\n",
    "precision, recall , _, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=[0, 1, 2, 3, 4, 5])\n",
    "pr_table = pd.DataFrame(index=range(0, 6), columns=['precision', 'recall'])\n",
    "for i in range(0, n_states):\n",
    "    pr_table.iloc[i, :] = [precision[i], recall[i]]\n",
    "pr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
