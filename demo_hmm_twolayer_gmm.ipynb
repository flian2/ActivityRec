{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from clean_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for person 4. Use sequence 1~3 for training, 4~5 for testing.\n",
    "person = 3\n",
    "sadl_n = []\n",
    "for n in range(1, 6):\n",
    "    sadl_n.append(pd.read_table('data/S%d-ADL%d.dat' % (person, n), sep='\\s+', header=None, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample size:  (11469, 36)\n",
      "train_labels size:  (11469,)\n",
      "subsequence length:  [4652 3478 3339] . Sum of length:  11469\n",
      "test_sample size:  (6771, 36)\n",
      "test_labels size:  (6771,)\n",
      "subsequence length:  [3460 3311] . Sum of length:  6771\n"
     ]
    }
   ],
   "source": [
    "# Smooth data, time: col 0, features: col 1~36, labels: col 244 \n",
    "winsize = 15\n",
    "stepsize = 8\n",
    "\n",
    "# train data\n",
    "train_sample = np.empty((0, 36))\n",
    "train_labels = np.empty((0))\n",
    "train_len = []\n",
    "for i in range(0, 3):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    train_sample = np.concatenate((train_sample, features), axis=0)\n",
    "    train_len.append(features.shape[0])\n",
    "    train_labels = np.concatenate( (train_labels, labels) )\n",
    "train_len = np.array(train_len)\n",
    "\n",
    "print \"train_sample size: \", train_sample.shape\n",
    "print \"train_labels size: \", train_labels.shape\n",
    "print \"subsequence length: \", train_len, \". Sum of length: \", np.sum(train_len)\n",
    "\n",
    "# test data\n",
    "test_sample = np.empty((0, 36))\n",
    "test_labels = np.empty((0))\n",
    "test_len = []\n",
    "for i in range(3, 5):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    test_sample = np.concatenate((test_sample, features), axis=0)\n",
    "    test_len.append(features.shape[0])\n",
    "    test_labels = np.concatenate( (test_labels, labels) )\n",
    "test_len = np.array(test_len)  \n",
    "\n",
    "print \"test_sample size: \", test_sample.shape\n",
    "print \"test_labels size: \", test_labels.shape\n",
    "print \"subsequence length: \", test_len, \". Sum of length: \", np.sum(test_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values\n",
    "col_threshold = 0.5\n",
    "train, test = fill_missing(train_sample, test_sample, col_threshold, True)\n",
    "np.any(np.isnan(train)), np.any(np.isnan(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler() # center to mean and normalize to unit variance\n",
    "train_normalized = scalar.fit_transform(train)\n",
    "test_normalized = scalar.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 21 compoments to retrain 0.950000 variance\n",
      "Size of reduced dimension training data:  (11469, 21)\n",
      "Size of reduced dimension testing data:  (6771, 21)\n"
     ]
    }
   ],
   "source": [
    "# Dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_normalized)\n",
    "var_thres = 0.95 # keep components to up to 95% total variance\n",
    "n_comp = (pca.explained_variance_ratio_.cumsum() < var_thres).sum() + 1\n",
    "print \"Keep %d compoments to retrain %f variance\" % (n_comp, var_thres)\n",
    "\n",
    "pca_train = PCA(n_components=n_comp)\n",
    "train_reduced = pca_train.fit_transform(train_normalized)\n",
    "test_reduced = pca_train.transform(test_normalized)\n",
    "print \"Size of reduced dimension training data: \", train_reduced.shape\n",
    "print \"Size of reduced dimension testing data: \", test_reduced.shape\n",
    "# Cleaned data: train_reduced, test_reduced, train_labels, test_labels, train_len, test_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize top-level left-right hmm model with 6 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability mass P(label | state): \n",
      "[[ 0.52196969  0.47803031  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.99999941  0.          0.        ]\n",
      " [ 0.          0.          0.99488235  0.          0.          0.        ]\n",
      " [ 0.03156406  0.          0.          0.          0.          0.96843594]\n",
      " [ 0.          0.          0.          0.          0.99999964  0.        ]\n",
      " [ 0.67153284  0.32846716  0.          0.          0.          0.        ]]\n",
      "The maximum likelihood label for each state: \n",
      "[0 3 2 5 4 0]\n"
     ]
    }
   ],
   "source": [
    "from hmm import DiscreteDistr, GaussDistr, GaussMixDistr\n",
    "from hmm import MarkovChain\n",
    "from hmm import HMM, make_leftright_hmm\n",
    "\n",
    "# Build a left-right hmm with 6 states, discrete output distribution, the output is the discrete label of activity\n",
    "label_transfer = (np.maximum(train_labels - 100, 0) + 1)[:, np.newaxis]\n",
    "# transform the labels into range 1~6. {0: 1, 101: 2, 102: 3, 103: 4, 104: 5, 105: 6}\n",
    "discreteD = DiscreteDistr(np.ones((6))) # a discrete distribution with 6 possible output\n",
    "n_states = 6\n",
    "\n",
    "hmm_state = make_leftright_hmm(n_states, discreteD, obs_data=label_transfer, l_data=train_len)\n",
    "print \"Probability mass P(label | state): \"\n",
    "prob_mass = np.zeros((n_states, n_states))\n",
    "for i in range(0, n_states):\n",
    "    prob_mass[i, :] = hmm_state.output_distr[i].prob_mass\n",
    "prob_mass[prob_mass < 1e-2] = 0\n",
    "for i in range(0, n_states):\n",
    "    hmm_state.output_distr[i].prob_mass = prob_mass[i, :]\n",
    "print prob_mass\n",
    "\n",
    "# Assign max probability activity label to each state\n",
    "# If multiple states are assigned with one same label, then inside each of these states use a small hmm \n",
    "state_act_label = np.array([np.argmax(prob_mass[i, :]) for i in range(0, n_states)])\n",
    "act_label_count = np.zeros((n_states)) # how many states correspond to label i\n",
    "for i in range(0, n_states):\n",
    "    act_label_count[i] = np.sum(state_act_label == i)\n",
    "print \"The maximum likelihood label for each state: \"\n",
    "print state_act_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_mixture</th>\n",
       "      <th>mean_loglikelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>-18.39338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>-32.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.09447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>-16.46939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>-26.89283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>-27.01296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_mixture mean_loglikelihood\n",
       "0           7          -18.39338\n",
       "101         1            -32.038\n",
       "102         4          -22.09447\n",
       "103         4          -16.46939\n",
       "104         2          -26.89283\n",
       "105         1          -27.01296"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train output distribution\n",
    "def search_num_mixtures_likelihood(features, train_len, n_mix):\n",
    "    # Use cross validation on loglikehood to find the optimal number of mixtures for each class label\n",
    "    # return: \n",
    "    # gmm: the gmm model that gives the highest mean logprob\n",
    "    # N: number of gaussian mixtures in the gmm\n",
    "    # logp_x: mean of logP(Xt | gmm) for the chosen gmm\n",
    "    mean_logprob = [0 for i in range(0, len(n_mix))] # mean of logP(Xt | gmm)\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    for i, N in enumerate(n_mix):\n",
    "        # K-fold cross validataion, each fold is one subsequence\n",
    "        n_folds = len(train_len)\n",
    "        likelihood_scores = []\n",
    "        for k in range(0, n_folds):\n",
    "            val_mask = np.zeros((features.shape[0])).astype(bool)\n",
    "            val_mask[start_ind[k]: start_ind[k] + train_len[k]] = True\n",
    "            train_mask = np.logical_not(val_mask)\n",
    "            gmm = GaussMixDistr(gauss=N)\n",
    "            gmm.init_by_data(features[train_mask, :])\n",
    "            gmm.train(features[train_mask, :])\n",
    "            logprob_x = gmm.logprob(features[val_mask, :])\n",
    "            likelihood_scores.append(np.mean(logprob_x))    \n",
    "        mean_logprob[i] = np.mean(likelihood_scores)\n",
    "    # Refit gmm using all data with selected number of mixtures\n",
    "    gmm_list = [] # one gmm model for each \"number of components\"\n",
    "    i_gmm = np.argmax(mean_logprob)\n",
    "    n_components = n_mix[i_gmm]\n",
    "    gmm_opt = GaussMixDistr(gauss=n_components)\n",
    "    gmm_opt.init_by_data(features)\n",
    "    gmm_opt.train(features)\n",
    "    likelihood_score = np.mean(gmm_opt.logprob(features))\n",
    "    return gmm_opt, n_components, likelihood_score\n",
    "\n",
    "def make_outputdistr(train, train_len, train_labels, class2label):\n",
    "    outputdistr_stats = pd.DataFrame(index=class2label, columns=['n_mixture', 'mean_loglikelihood'])\n",
    "    outputdistr_gmm = []\n",
    "    n_mix = range(1, 11)\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    for i, label in enumerate(class2label):\n",
    "        per_label_trainlen = [sum(train_labels[start_ind[k]: start_ind[k] + length] == label) for k, length in enumerate(train_len)]\n",
    "        gmm, N, logprob = search_num_mixtures_likelihood(train[train_labels == label], per_label_trainlen, n_mix)\n",
    "        outputdistr_gmm.append(gmm)\n",
    "        outputdistr_stats.iloc[i, :] = [N, logprob]\n",
    "    return outputdistr_gmm, outputdistr_stats\n",
    "\n",
    "class2label = [0, 101, 102, 103, 104, 105]\n",
    "outputdistr_gmm, outputdistr_stats = make_outputdistr(train_reduced, train_len, train_labels, class2label)\n",
    "outputdistr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglikelihood(hmm_top, x, gmms):\n",
    "    \"\"\"\n",
    "    Compute log likelihood for each observation sample given the MarkovChain of top-layer hmm, \n",
    "    and gmm distribution P(X(t) | activity label = i).\n",
    "    Input:\n",
    "    hmm_top: top-layer hmm\n",
    "    gmms: list of gmm object. gmms[i] has P(X(t) | activity_label = i)\n",
    "    Return:\n",
    "    logp_x: [n_states, n_samples]. logp_x[i, t] = log P[X(t) | S_{hmm_top} = i]\n",
    "    Method:\n",
    "    Compute: P(X(t) | state = i ) for t = 0...T, i = 0...5,\n",
    "    P(X(t) | state = i ) = \\sum_{label i} P(X(t) | label = j, state = i) * P(label = j | state = i)\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    n_states = hmm_top.n_states\n",
    "    logp_x = np.zeros((n_states, T))\n",
    "    for state in range(0, hmm_top.n_states):\n",
    "        p0 = hmm_top.output_distr[state].prob_mass\n",
    "        label_ind = np.argwhere(p0 > 0)\n",
    "        p0 = p0[p0 > 0]\n",
    "        logprob_per_label = gmms[0].logprob(x, [gmms[i] for i in label_ind])\n",
    "        logp_x[state, :] = np.log( p0[np.newaxis, :].dot(np.exp(logprob_per_label)) )[0, :]\n",
    "\n",
    "    return logp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi_state_sequence(hmm_top, x, x_len, gmms):\n",
    "    \"\"\"\n",
    "    Predict top-layer hmm state sequence using viterbi algorithm.\n",
    "    Input\n",
    "    hmm_top: top-layer hmm\n",
    "             hmm_top.state_gen: MarkovChain for top-layer hmm.\n",
    "             hmm_top.output_distr: hmm_top.output_distr[i][j] = P[activity_label = j | S_{hmm_top} = i]\n",
    "    x: [T, data_size]. observation vector sequence stacked together\n",
    "    x_len: length of subsequences\n",
    "    gmms:      list of gmm object. gmms[i] has P(X(t) | activity_label = i)\n",
    "    Return:\n",
    "    s_opt: [T, ] predicted top-layer hmm state sequence\n",
    "    logP: [n_seq, ] logP of each subsequence\n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    s_opt = np.zeros((x.shape[0]))\n",
    "    logP = np.zeros((len(x_len)))\n",
    "    for i in range(0, len(x_len)):\n",
    "        logp_x = loglikelihood(hmm_top, x[start_ind:start_ind + x_len[i], :], gmms)\n",
    "        s_opt[start_ind: start_ind + x_len[i]], logP[i] = hmm_top.state_gen.viterbi(logp_x)\n",
    "        start_ind += x_len[i]\n",
    "    return s_opt - 1, logP # the state sequence index from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict label within each state\n",
    "# Make an ergordic hmm with output distribution 1.0, (1-to-1 mapping between label and substate)\n",
    "\n",
    "train_states, _ = hmm_state.viterbi(label_transfer)\n",
    "train_states -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sub_hmm_mc(train_states, train_len, train_labels, n_states):\n",
    "    \"\"\"\n",
    "    Initialize and train the sub-layer hmm for each top-layer hmm state.\n",
    "    For the sub-layer hmm, each state corresponds to one activity label. \n",
    "    We train the transition prob by running Baum-Weltch EM training, by initializing the output \n",
    "    probability mass as the diagonal matrix, to force one to one mapping between sub-layer state and label.\n",
    "    Input:\n",
    "    train_states: [n_samples, ]. Sequence of top-layer hmm states.\n",
    "    train_len: list. Length of subsequences.\n",
    "    train_labels: [n_samples, ]. Activity labels of training sequence. The range of each label must be in [0, n_states)\n",
    "    Return:\n",
    "    mc_per_state: list of MarkovChain objects. mc_per_state[i] is the MarkovChain for top-layer state i. \n",
    "    \"\"\"\n",
    "    train_labels = train_labels[:, np.newaxis]\n",
    "    start_ind = [0] + list(np.cumsum(train_len)[:-1].astype(int))\n",
    "    mc_per_state = []\n",
    "    for n in range(0, n_states):\n",
    "        x_labels = train_labels[train_states == n, :]\n",
    "        x_len = np.array([np.sum(train_states[s:s + train_len[i]] == n) for i, s in enumerate(start_ind)])\n",
    "        x_len = x_len[x_len > 0]\n",
    "        # Aii = 1 - 1/state_duration, Aij = 1/state_duration / (n_states_actual - 1)\n",
    "        A0 = np.eye((n_states))\n",
    "        D = np.array([np.sum(x_labels == i) / float(len(x_len)) for i in range(0, n_states)])\n",
    "        n_states_actual = np.sum(D > 0)\n",
    "        for i in range(0, n_states):\n",
    "            if D[i] == 0:\n",
    "                continue\n",
    "            A0[i, D > 0] = 1.0 / D[i] / n_states_actual\n",
    "            A0[i, i] = 1.0 - 1.0 / D[i]\n",
    "        p0 = np.ones((n_states)) / float(n_states_actual)\n",
    "        p0[D == 0] = 0.0\n",
    "        prob_mass = np.eye((n_states))\n",
    "        pD = [DiscreteDistr(prob_mass[i, :]) for i in range(0, n_states)]\n",
    "        mc = MarkovChain(p0, A0)\n",
    "        hmm_mc = HMM(mc, pD)\n",
    "        hmm_mc.train(obs_data=x_labels + 1, l_data=x_len) # discrete distribution index from 1 \n",
    "        mc_per_state.append(hmm_mc.state_gen)\n",
    "    return mc_per_state\n",
    "    \n",
    "sub_mcs = make_sub_hmm_mc(train_states, train_len, np.maximum(0, train_labels - 100), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict activity labels inside each high-level hmm state\n",
    "def predict_subhmm_labels(states_tophmm, obs_data, l_data, sub_mcs, outputdistr):\n",
    "    \"\"\"\n",
    "    Predict activity labels inside each top-layer hmm state.\n",
    "    Input:\n",
    "    states_tophmm: [n_samples, ]. Sequences of top-layer hmm states.\n",
    "    obs_data: [n_samples, n_features]\n",
    "    l_data: length of sequences. sum(l_data) = len(states_tophmm)\n",
    "    sub_mcs: MarkovChain objects for each top-layer state. The order of states in the mc corresponds to the sub-level label.\n",
    "    outputdistr: the output distribution for the states in sub_mcs.\n",
    "    Return:\n",
    "    labels_opt: [n_samples, ]. predicted labels, range from 0 to n_states - 1.\n",
    "    \"\"\"\n",
    "    start_ind = [0] + list(np.cumsum(l_data)[:-1].astype(int))\n",
    "    labels_opt = np.zeros((states_tophmm.shape[0]))\n",
    "    cur_pos = 0\n",
    "    for t in range(0, len(l_data)):\n",
    "        state_subseq = states_tophmm[start_ind[t]:start_ind[t] + l_data[t]]\n",
    "        obs_subseq = obs_data[start_ind[t]:start_ind[t] + l_data[t], :]\n",
    "        diff = np.append(np.array([1]), state_subseq[1:] - state_subseq[:-1])\n",
    "        i_newstate = np.append( np.argwhere(diff != 0), np.array([len(diff)]) )\n",
    "        for m in range(0, len(i_newstate) - 1):\n",
    "            state_tt = int(state_subseq[i_newstate[m]])\n",
    "            obs_tt = obs_subseq[i_newstate[m]:i_newstate[m + 1], :]\n",
    "            # run viterbi on markov chain sub_mcs[state_tt]\n",
    "            logp_x = outputdistr[0].logprob(obs_tt, outputdistr)\n",
    "            labels_tt, logP = sub_mcs[state_tt].viterbi(logp_x)\n",
    "            labels_opt[cur_pos:cur_pos + len(labels_tt)] = labels_tt - 1 # make the label index starting from 0\n",
    "            cur_pos += len(labels_tt)\n",
    "    return labels_opt\n",
    "\n",
    "# Predict test labels\n",
    "test_states, _ = viterbi_state_sequence(hmm_state, test_reduced, test_len, outputdistr_gmm)\n",
    "predicted_labels = predict_subhmm_labels(test_states, test_reduced, test_len, sub_mcs, outputdistr_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAEZCAYAAACZ7CwhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqTijyV8ppZSKM/8PTceK9aSjm/0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1035c19d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.853640525772\n"
     ]
    }
   ],
   "source": [
    "# Test with ADL4, ADL5\n",
    "true_labels = np.maximum(0, test_labels - 100)\n",
    "\n",
    "plt.plot(predicted_labels, '.', label=\"estimated label\")\n",
    "plt.plot(true_labels, '-', label=\"true label\")\n",
    "plt.plot(test_states, '--', label=\"high-level states\")\n",
    "plt.xlabel('time step')\n",
    "plt.ylabel('Activity label')\n",
    "plt.title(\"ADL4, ADL5 -- person %d\" % person)\n",
    "plt.legend(loc=9, bbox_to_anchor=(1.2, 1))\n",
    "plt.show()\n",
    "\n",
    "print \"accuracy: \", sum(predicted_labels == true_labels) / float(len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 score:  0.857081735691\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7580645</td>\n",
       "      <td>0.9121005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9598716</td>\n",
       "      <td>0.8616715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5719109</td>\n",
       "      <td>0.9988208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.99747</td>\n",
       "      <td>0.9962097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9679898</td>\n",
       "      <td>0.8235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9616307</td>\n",
       "      <td>0.649568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision     recall\n",
       "0  0.7580645  0.9121005\n",
       "1  0.9598716  0.8616715\n",
       "2  0.5719109  0.9988208\n",
       "3    0.99747  0.9962097\n",
       "4  0.9679898  0.8235294\n",
       "5  0.9616307   0.649568"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print \"Weighted F1 score: \", f1_score(true_labels, predicted_labels, average='weighted') \n",
    "\n",
    "precision, recall , _, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=[0, 1, 2, 3, 4, 5])\n",
    "pr_table = pd.DataFrame(index=range(0, 6), columns=['precision', 'recall'])\n",
    "for i in range(0, n_states):\n",
    "    pr_table.iloc[i, :] = [precision[i], recall[i]]\n",
    "pr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
