{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from clean_data import *\n",
    "from hmm import DiscreteDistr, GaussDistr, GaussMixDistr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for person 4. Use sequence 1~3 for training, 4~5 for testing.\n",
    "person = 3\n",
    "sadl_n = []\n",
    "for n in range(1, 6):\n",
    "    sadl_n.append(pd.read_table('data/S%d-ADL%d.dat' % (person, n), sep='\\s+', header=None, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample size:  (11469, 36)\n",
      "train_labels size:  (11469,)\n",
      "subsequence length:  [4652 3478 3339] . Sum of length:  11469\n",
      "test_sample size:  (6771, 36)\n",
      "test_labels size:  (6771,)\n",
      "subsequence length:  [3460 3311] . Sum of length:  6771\n"
     ]
    }
   ],
   "source": [
    "# Smooth data, time: col 0, features: col 1~36, labels: col 244 \n",
    "winsize = 15\n",
    "stepsize = 8\n",
    "\n",
    "# train data\n",
    "train_sample = np.empty((0, 36))\n",
    "train_labels = np.empty((0))\n",
    "train_len = []\n",
    "for i in range(0, 3):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    train_sample = np.concatenate((train_sample, features), axis=0)\n",
    "    train_len.append(features.shape[0])\n",
    "    train_labels = np.concatenate( (train_labels, labels) )\n",
    "train_len = np.array(train_len)\n",
    "\n",
    "print \"train_sample size: \", train_sample.shape\n",
    "print \"train_labels size: \", train_labels.shape\n",
    "print \"subsequence length: \", train_len, \". Sum of length: \", np.sum(train_len)\n",
    "\n",
    "# test data\n",
    "test_sample = np.empty((0, 36))\n",
    "test_labels = np.empty((0))\n",
    "test_len = []\n",
    "for i in range(3, 5):\n",
    "    features = moving_avg(sadl_n[i].iloc[:, 1:37], winsize, stepsize)\n",
    "    labels = moving_vote_majority(sadl_n[i].iloc[:, 244], winsize, stepsize)\n",
    "    test_sample = np.concatenate((test_sample, features), axis=0)\n",
    "    test_len.append(features.shape[0])\n",
    "    test_labels = np.concatenate( (test_labels, labels) )\n",
    "test_len = np.array(test_len)  \n",
    "\n",
    "print \"test_sample size: \", test_sample.shape\n",
    "print \"test_labels size: \", test_labels.shape\n",
    "print \"subsequence length: \", test_len, \". Sum of length: \", np.sum(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "col_threshold = 0.5\n",
    "train, test = fill_missing(train_sample, test_sample, col_threshold, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train gmm using raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train gmm using raw features\n",
    "\n",
    "def make_outputdistr(train, train_labels, class2label):\n",
    "    outputdistr_stats = pd.DataFrame(index=class2label, columns=['n_mixture', 'mean_logprob'])\n",
    "    outputdistr_gmm = []\n",
    "    n_mix = range(1, 10)\n",
    "    for i, label in enumerate(class2label):\n",
    "        gmm, N, logprob = search_num_mixtures(train[train_labels == label], n_mix)\n",
    "        outputdistr_gmm.append(gmm)\n",
    "        outputdistr_stats.iloc[i, :] = [N, logprob]\n",
    "    return outputdistr_gmm, outputdistr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def search_num_mixtures(features, n_mix):\n",
    "    # return: \n",
    "    # gmm: the gmm model that gives the highest mean logprob\n",
    "    # N: number of gaussian mixtures in the gmm\n",
    "    # logp_x: mean of logP(Xt | gmm) for the chosen gmm\n",
    "    mean_logprob = [0 for i in range(0, len(n_mix))] # mean of logP(Xt | gmm)\n",
    "\n",
    "    for i, N in enumerate(n_mix):\n",
    "        # K-fold cross validataion\n",
    "        n_folds = 3\n",
    "        kf = KFold(n_splits = n_folds, random_state=0)\n",
    "        likelihood_scores = []\n",
    "        for train, val in kf.split(features):\n",
    "            gmm = GaussMixDistr(gauss=N)\n",
    "            gmm.init_by_data(features[train, :])\n",
    "            gmm.train(features[train, :])\n",
    "            logprob_x = gmm.logprob(features[val, :])\n",
    "            likelihood_scores.append(np.mean(logprob_x))    \n",
    "        mean_logprob[i] = np.mean(likelihood_scores)\n",
    "    # Refit gmm using all data with selected number of mixtures\n",
    "    gmm_list = [] # one gmm model for each \"number of components\"\n",
    "    i_gmm = np.argmax(mean_logprob)\n",
    "    n_components = n_mix[i_gmm]\n",
    "    gmm_opt = GaussMixDistr(gauss=n_components)\n",
    "    gmm_opt.init_by_data(features)\n",
    "    gmm_opt.train(features)\n",
    "    likelihood_score = np.mean(gmm_opt.logprob(features))\n",
    "    \n",
    "    return gmm_opt, n_components, likelihood_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_mixture</th>\n",
       "      <th>mean_logprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>-250.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6</td>\n",
       "      <td>-228.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>9</td>\n",
       "      <td>-221.7732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>-223.4718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5</td>\n",
       "      <td>-239.5752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>9</td>\n",
       "      <td>-232.8739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_mixture mean_logprob\n",
       "0           7     -250.095\n",
       "101         6    -228.8917\n",
       "102         9    -221.7732\n",
       "103         4    -223.4718\n",
       "104         5    -239.5752\n",
       "105         9    -232.8739"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputdistr_gmm, outputdistr_stats = make_outputdistr(train, train_labels, [0, 101, 102, 103, 104, 105])\n",
    "outputdistr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evalute gmm on test data using naive bayes classifier\n",
    "# max over label: P(label | X_t) = P(label, Xt) / P(X_t) ~ P(Xt | label) * P(label)\n",
    "\n",
    "def naive_bayes_gmm_predict(val, gmms, priors, class2label):\n",
    "    # Input\n",
    "    # gmms: list of gmm distributions\n",
    "    # priors: [n_states, ] prior probabilities\n",
    "    # Returns:\n",
    "    # labels\n",
    "    n_samples = val.shape[0]\n",
    "    logp_x_given_label = gmms[0].logprob(val, gmms)\n",
    "    logp0 = np.tile(np.log(priors)[:, np.newaxis], (1, n_samples))\n",
    "    logp_posterior = logp_x_given_label + logp0\n",
    "    i_map = np.argmax(logp_posterior, axis=0)\n",
    "    return [class2label[i] for i in i_map]\n",
    "\n",
    "def compute_priors(train_labels, class2label):\n",
    "    return [np.sum(train_labels == class2label[i]) / float(len(train_labels)) for i in range(0, len(class2label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priors:  [0.14962071671462202, 0.094254076205423309, 0.16235068445374487, 0.2370738512511989, 0.096085098962420443, 0.26061557241259048]\n",
      "accuracy on train data:  0.714186066789\n",
      "accuracy on test data:  0.590902377788\n"
     ]
    }
   ],
   "source": [
    "class2label = [0, 101, 102, 103, 104, 105]\n",
    "priors = compute_priors(train_labels, class2label)\n",
    "print \"priors: \", priors\n",
    "train_label_pred = naive_bayes_gmm_predict(train, outputdistr_gmm, priors, class2label)\n",
    "print \"accuracy on train data: \", np.sum(train_labels == train_label_pred) / float(len(train_label_pred))\n",
    "\n",
    "test_label_pred = naive_bayes_gmm_predict(test, outputdistr_gmm, priors, class2label)\n",
    "print \"accuracy on test data: \", np.sum(test_labels == test_label_pred) / float(len(test_label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train gmm using feature normalization and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler() # center to mean and normalize to unit variance\n",
    "train_normalized = scalar.fit_transform(train)\n",
    "test_normalized = scalar.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_mixture</th>\n",
       "      <th>mean_logprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-44.49249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3</td>\n",
       "      <td>-57.33226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>8</td>\n",
       "      <td>-17.56501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>-16.02311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3</td>\n",
       "      <td>-35.01792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7</td>\n",
       "      <td>-25.54102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_mixture mean_logprob\n",
       "0           5    -44.49249\n",
       "101         3    -57.33226\n",
       "102         8    -17.56501\n",
       "103         4    -16.02311\n",
       "104         3    -35.01792\n",
       "105         7    -25.54102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train gmm using normalized features\n",
    "gmms_normalized, gmms_normalized_stats = make_outputdistr(train_normalized, train_labels, [0, 101, 102, 103, 104, 105])\n",
    "gmms_normalized_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.707908274479\n",
      "accuracy on test data:  0.589573179737\n"
     ]
    }
   ],
   "source": [
    "train_label_pred = naive_bayes_gmm_predict(train_normalized, gmms_normalized, priors, class2label)\n",
    "print \"accuracy on train data: \", np.sum(train_labels == train_label_pred) / float(len(train_label_pred))\n",
    "\n",
    "test_label_pred = naive_bayes_gmm_predict(test_normalized, gmms_normalized, priors, class2label)\n",
    "print \"accuracy on test data: \", np.sum(test_labels == test_label_pred) / float(len(test_label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 21 compoments to retrain 0.950000 variance\n",
      "Size of reduced dimension training data:  (11469, 21)\n",
      "Size of reduced dimension testing data:  (6771, 21)\n"
     ]
    }
   ],
   "source": [
    "# Dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_normalized)\n",
    "var_thres = 0.95 # keep components to up to 95% total variance\n",
    "n_comp = (pca.explained_variance_ratio_.cumsum() < var_thres).sum() + 1\n",
    "print \"Keep %d compoments to retrain %f variance\" % (n_comp, var_thres)\n",
    "\n",
    "pca_train = PCA(n_components=n_comp)\n",
    "pca_train.fit(train_normalized)\n",
    "train_reduced = pca_train.transform(train_normalized)\n",
    "test_reduced = pca_train.transform(test_normalized)\n",
    "print \"Size of reduced dimension training data: \", train_reduced.shape\n",
    "print \"Size of reduced dimension testing data: \", test_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_mixture</th>\n",
       "      <th>mean_logprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-14.4257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>-12.76097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>-23.11768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>-16.46831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>-26.89163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>-24.83486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_mixture mean_logprob\n",
       "0           5     -14.4257\n",
       "101         5    -12.76097\n",
       "102         3    -23.11768\n",
       "103         4    -16.46831\n",
       "104         2    -26.89163\n",
       "105         3    -24.83486"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmms_reduced, gmms_reduced_stats = make_outputdistr(train_reduced, train_labels, [0, 101, 102, 103, 104, 105])\n",
    "gmms_reduced_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.718720027901\n",
      "accuracy on test data:  0.616452518092\n"
     ]
    }
   ],
   "source": [
    "train_label_pred = naive_bayes_gmm_predict(train_reduced, gmms_reduced, priors, class2label)\n",
    "print \"accuracy on train data: \", np.sum(train_labels == train_label_pred) / float(len(train_label_pred))\n",
    "\n",
    "test_label_pred = naive_bayes_gmm_predict(test_reduced, gmms_reduced, priors, class2label)\n",
    "print \"accuracy on test data: \", np.sum(test_labels == test_label_pred) / float(len(test_label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
